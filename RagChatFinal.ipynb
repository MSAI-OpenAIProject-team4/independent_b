{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install openai pandas scikit-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ğŸ¯ ì„ë² ë”©ìš© Azure í´ë¼ì´ì–¸íŠ¸\n",
    "embedding_client = AzureOpenAI(\n",
    "  api_key = \"\",\n",
    "  api_version = \"\",\n",
    "  azure_endpoint = \"\"\n",
    ")\n",
    "\n",
    "\n",
    "# ğŸ¯ ì±—ìš© Azure í´ë¼ì´ì–¸íŠ¸\n",
    "chat_client = AzureOpenAI(\n",
    "  api_key=\"\",\n",
    "  api_version=\"\",\n",
    "  azure_endpoint=\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#âœ… 2. CSV ë¡œë”© í•¨ìˆ˜\n",
    "\n",
    "def load_embedded_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['ada_v2'] = df['ada_v2'].apply(lambda x: eval(x) if isinstance(x, str) else x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#âœ… 3. ì„ë² ë”© í•¨ìˆ˜\n",
    "def generate_embeddings(text, model=\"text-embedding-ada-002\"):\n",
    "    response = embedding_client.embeddings.create(\n",
    "        input=[text],\n",
    "        model=model\n",
    "    )\n",
    "    return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#âœ… 4. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ / name ì¸ì‹ / í•„í„°ë¥¼ ì¶”ê°€ / ì´ë¦„ ì¶œë ¥(ìµœì¢…)\n",
    "\n",
    "def filter_df_by_query(query, df):\n",
    "    # â‘  ì´ë¦„ ê¸°ë°˜ í•„í„° ìš°ì„  ì ìš©\n",
    "    name_filtered = df[df['name'].str.contains(query, na=False)]\n",
    "\n",
    "    if not name_filtered.empty:\n",
    "        print(\"âœ… ì´ë¦„ ê¸°ë°˜ í•„í„°ë§ ì ìš©:\", name_filtered['name'].tolist())\n",
    "        return name_filtered\n",
    "\n",
    "    # â‘¡ ë‹¤ì¤‘ í•„ë“œ ê¸°ë°˜ í•„í„°\n",
    "    multi_filtered = df[\n",
    "        df['orders'].str.contains(query, na=False) |\n",
    "        df['movementFamily'].str.contains(query, na=False) |\n",
    "        df['activities'].str.contains(query, na=False) |\n",
    "        df['content'].str.contains(query, na=False)\n",
    "    ]\n",
    "\n",
    "    if not multi_filtered.empty:\n",
    "        print(\"âœ… ë‹¤ì¤‘ í•„ë“œ ê¸°ë°˜ í•„í„°ë§ ì ìš©\")\n",
    "        return multi_filtered\n",
    "\n",
    "    print(\"âš ï¸ í•„í„°ë§ ì‹¤íŒ¨ â†’ ì „ì²´ ë°ì´í„° ì‚¬ìš©\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def retrieve_relevant_context(query, df, top_k=3, embed_model=\"text-embedding-ada-002\"):\n",
    "    # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„±\n",
    "    query_emb = generate_embeddings(query, model=embed_model)\n",
    "\n",
    "    # 2. ìœ ì‚¬ë„ ê³„ì‚°\n",
    "    df['score'] = df['ada_v2'].apply(lambda x: cosine_similarity([x], [query_emb])[0][0])\n",
    "\n",
    "    # âœ… 3. ì´ë¦„ í¬í•¨ëœ ë¬¸ì„œì—ëŠ” ì ìˆ˜ ê°€ì¤‘ì¹˜ ì¶”ê°€\n",
    "    df.loc[df['name'].str.contains(query, na=False), 'score'] += 1.0\n",
    "\n",
    "    # 4. Top-K ë¬¸ì„œ ì„ íƒ\n",
    "    top_docs = df.sort_values(by=\"score\", ascending=False).head(top_k).reset_index()\n",
    "\n",
    "    context_blocks = []\n",
    "    citations = []\n",
    "\n",
    "    print(\"ğŸ” ìœ ì‚¬ë„ ìƒìœ„ ë¬¸ì„œ (Top {}):\".format(top_k))\n",
    "    for i, row in top_docs.iterrows():\n",
    "        ref_id = f\"[{i+1}]\"\n",
    "        name = row.get(\"name\", \"\")\n",
    "        score = round(row[\"score\"], 4)\n",
    "        print(f\"{ref_id} {name} (score: {score})\")\n",
    "\n",
    "        # GPTì— ë„˜ê¸¸ ë¬¸ì„œ ë¸”ë¡ êµ¬ì„±\n",
    "        text = f\"\"\"{ref_id}\n",
    "        ì´ë¦„: {name}\n",
    "        í•œì: {row.get(\"nameHanja\", \"\")}\n",
    "        ì¶œìƒì§€: {row.get(\"addressBirth\", \"\")}\n",
    "        í›ˆê²©: {row.get(\"orders\", \"\")}\n",
    "        ë…ë¦½ìš´ë™ ë¶„ì•¼: {row.get(\"movementFamily\", \"\")}\n",
    "        ì£¼ìš” í™œë™: {row.get(\"activities\", \"\")}\n",
    "        ê´€ë ¨ ë‹¨ì²´: {row.get(\"engagedOrganizations\", \"\")}\n",
    "        ë‚´ìš©: {row['content']}\n",
    "        \"\"\"\n",
    "        context_blocks.append(text)\n",
    "\n",
    "        # Citation êµ¬ì„±\n",
    "        citation_info = {\n",
    "            \"index\": i + 1,\n",
    "            \"title\": name,\n",
    "            \"reference\": row.get(\"references\")\n",
    "        }\n",
    "        citations.append(citation_info)\n",
    "\n",
    "    context = \"\\n\\n\".join(context_blocks)\n",
    "    return context, citations\n",
    "\n",
    "\n",
    "def rag_chatbot(\n",
    "    query,\n",
    "    df,\n",
    "    top_k=5,\n",
    "    embed_model=\"text-embedding-ada-002\",\n",
    "    chat_model=\"gpt-4o\",\n",
    "    system_prompt=\"ë„ˆëŠ” ë…ë¦½ìš´ë™ ì „ë¬¸ê°€ì•¼. ë¬¸ì„œ ë‚´ 'ì´ë¦„'ê³¼ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¹„êµí•´ì„œ ì •í™•íˆ ë‹µë³€í•´ì¤˜.\"\n",
    "):\n",
    "    # 1. í•„í„°ë§\n",
    "    filtered_df = filter_df_by_query(query, df)\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(\"âš ï¸ ì´ë¦„/ë‚´ìš© í•„í„°ë§ ì‹¤íŒ¨ â†’ ì „ì²´ ë°ì´í„° ì‚¬ìš©\")\n",
    "        filtered_df = df\n",
    "\n",
    "    # 2. ìœ ì‚¬ë„ ê¸°ë°˜ context ìƒì„±\n",
    "    context, citations = retrieve_relevant_context(query, filtered_df, top_k=top_k, embed_model=embed_model)\n",
    "    \n",
    "    # 3. GPT í˜¸ì¶œ\n",
    "    response = chat_client.chat.completions.create(\n",
    "        model=chat_model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"ì§ˆë¬¸: {query}\\n\\nì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ëŒ€ë‹µí•´ì¤˜:\\n\\n{context}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 4. ê²°ê³¼ ì¶œë ¥\n",
    "    content_text = response.choices[0].message.content\n",
    "\n",
    "    print(\"ğŸ¤– GPT ì‘ë‹µ:\\n\")\n",
    "    print(content_text)\n",
    "\n",
    "    print(\"\\nğŸ“š Citations:\")\n",
    "    for c in citations:\n",
    "        print(f\"[{c['index']}] {c['title']}\\nì¶œì²˜: {c['reference']}\\n\")\n",
    "\n",
    "    return content_text, citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#âœ… 5. ì±— ì‘ë‹µ í•¨ìˆ˜\n",
    "def ask_gpt(query, context, model=\"gpt-4o\",system_prompt=\"ë„ˆëŠ” í•œêµ­ ë…ë¦½ìš´ë™ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì •í™•í•˜ê²Œ ëŒ€ë‹µí•´ì¤˜.\"):\n",
    "    response = chat_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"ì§ˆë¬¸: {query}\\n\\nì°¸ê³  ë¬¸ì„œ:\\n{context}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"ì¸ë¬¼: {query}\\n\\nì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ì´ ì¸ë¬¼ì— ëŒ€í•œ ë‚´ìš©ì„ ì°¾ì•„ì¤˜.\\n\\n{context}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# âœ… ì‚¬ìš© ì˜ˆì‹œ\n",
    "response, citations = rag_chatbot(\n",
    "    query=\"ì• êµ­ì¥ê³¼ ê´€ë ¨ìˆëŠ” ë…ë¦½ìš´ë™ê°€ 3ì¸?\",\n",
    "    df=df,\n",
    "    embed_model=\"text-embedding-ada-002\",\n",
    "    chat_model=\"gpt-4o\",\n",
    "    top_k=5,  # â† top_kë¥¼ ëŠ˜ë¦¬ë©´ ë” ë„“ì€ ë¬¸ì„œ ë²”ìœ„ë¥¼ ê²€ìƒ‰í•¨\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    ë„ˆëŠ” í•œêµ­ ë…ë¦½ìš´ë™ ì¸ë¬¼ ì „ë¬¸ AIì•¼. ì¹œì ˆí•˜ê³  ìƒë™ê°ìˆê²Œ ì´ì•¼ê¸°í•˜ë“¯ ë§í•´ì¤˜. ìœ ì‚¬ë„ ë†’ì€ ì¸ë¬¼ ì¤‘ì‹¬ìœ¼ë¡œ ë‚´ìš©ì„ ëŒ€ë‹µí•´.\n",
    "    ë¬¸ì„œ ì¤‘ 'ê´€ë ¨ ì¸ë¬¼', 'í™œë™ ë‚´ì—­'ì—ì„œ ì—°ê²°ì ì„ ì°¾ì•„  \n",
    "    ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¸ ì¸ë¬¼ê³¼ ì—°ê´€ëœ ë‹¤ë¥¸ ì¸ë¬¼ì´ ìˆë‹¤ë©´ ì´ë¦„ì„ ëª…í™•íˆ ì•Œë ¤ì¤˜.\n",
    "\n",
    "    - ë°˜ë“œì‹œ ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ë§Œ ì°¸ê³ í•´ì„œ ëŒ€ë‹µí•´.  \n",
    "    - ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” \"ë¬¸ì„œì— í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ë¶„ëª…íˆ ë§í•´ì¤˜.  \n",
    "    - ì¶”ë¡ í•˜ê±°ë‚˜ ì™¸ë¶€ ì§€ì‹ì„ ì¶”ê°€í•˜ì§€ ë§ˆ.  \n",
    "    - ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì¸ë¬¼, í™œë™, ìƒí›ˆ, ì†Œì† ë‹¨ì²´, ê´€ë ¨ ì¸ë¬¼ì— ëŒ€í•œ ê²ƒì´ë©´ í•´ë‹¹ ë¬¸ì„œ í•„ë“œë¥¼ ì°¾ì•„ì„œ ì„¤ëª…í•´ì¤˜.\n",
    "\n",
    "    ì—°ê²°ëœ ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ì—°ê´€ëœ ì¸ë¬¼ ì •ë³´ëŠ” ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•´ì¤˜.\n",
    "    \"\"\"\n",
    "\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
